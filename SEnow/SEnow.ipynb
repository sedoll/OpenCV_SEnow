{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bda69210",
   "metadata": {},
   "source": [
    "# 웹캠 연동해서 만든 SEnow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd39b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "#이미지 불러오기\n",
    "imageList = [[cv2.imread('right_eye_cutout.png', cv2.IMREAD_UNCHANGED), cv2.imread('left_eye_cutout.png', cv2.IMREAD_UNCHANGED),\n",
    "              cv2.imread('nose_tip_cutout.png', cv2.IMREAD_UNCHANGED)],\n",
    "            [cv2.imread('right_eye2.png', cv2.IMREAD_UNCHANGED), cv2.imread('left_eye2.png', cv2.IMREAD_UNCHANGED),\n",
    "              cv2.imread('nose_tip2.png', cv2.IMREAD_UNCHANGED)],\n",
    "            [cv2.imread('right_eye3.png', cv2.IMREAD_UNCHANGED), cv2.imread('left_eye3.png', cv2.IMREAD_UNCHANGED),\n",
    "              cv2.imread('nose_tip3.png', cv2.IMREAD_UNCHANGED)]]\n",
    "\n",
    "# 이미지 기본값은 판다\n",
    "image_right_eye = imageList[0][0]\n",
    "image_left_eye = imageList[0][1]\n",
    "image_nose_tip = imageList[0][2]\n",
    "\n",
    "# For static images:\n",
    "IMAGE_FILES = []\n",
    "with mp_face_detection.FaceDetection(\n",
    "        model_selection=1, min_detection_confidence=0.5) as face_detection:\n",
    "    for idx, file in enumerate(IMAGE_FILES):\n",
    "        image = cv2.imread(file)\n",
    "        # Convert the BGR image to RGB and process it with MediaPipe Face Detection.\n",
    "        results = face_detection.process(\n",
    "            cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "        # Draw face detections of each face.\n",
    "        if not results.detections:\n",
    "            continue\n",
    "        annotated_image = image.copy()\n",
    "        for detection in results.detections:\n",
    "            print('Nose tip:')\n",
    "            print(mp_face_detection.get_key_point(\n",
    "                detection, mp_face_detection.FaceKeyPoint.NOSE_TIP))\n",
    "            mp_drawing.draw_detection(annotated_image, detection)\n",
    "        cv2.imwrite('/tmp/annotated_image' +\n",
    "                    str(idx) + '.png', annotated_image)\n",
    "\n",
    "# For webcam input:\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "SAVE_PATH = \"C:/senow/\"\n",
    "\n",
    "# 창 크기 출력\n",
    "w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "print(w, h) # 640, 480\n",
    "SCREEN_REGION = (0, 0, w, h)\n",
    "\n",
    "# 캠에 이미지 덮어 씌우는 함수\n",
    "def overlay(image, x, y, w, h, overlay_image): # 대상 이미지, x, y 좌표, width, height, 덮어씌울 이미지\n",
    "    alpha = overlay_image[:, :, 3] #BGRA, A값을 가져옴\n",
    "    mask_image = alpha / 255 # 0~255 ->255로 나누면 0~1의 값을 가짐, 1: 불투명, 0: 투명\n",
    "    \n",
    "    # 얼굴이 창 크기를 벗어나면 오류가 생기므로 예외처리\n",
    "    try:\n",
    "        for c in range(0, 3): #BGR 처리\n",
    "            image[y-h: y+h, x-w: x+w, c] = (overlay_image[:, :, c] * mask_image) + (image[y-h: y+h, x-w: x+w, c] * (1-mask_image))\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "# 이미지 저장 함수\n",
    "def displayCapture(screenshot): # screenshot을 통해 opencv 창 정보를 받아옴\n",
    "    \n",
    "    # 이미지 저장 폴더, 없는 경우 생성\n",
    "    if not os.path.exists(SAVE_PATH):\n",
    "        os.makedirs(SAVE_PATH)\n",
    "\n",
    "    try:\n",
    "        # 현재 시간을 파일 이름으로 사용하여 png 파일로 저장\n",
    "        current_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H-%M-%S\")\n",
    "        file_name = f\"{SAVE_PATH}/{current_time}.png\"\n",
    "        cv2.imwrite(file_name, screenshot) # 이미지 저장\n",
    "        print(f\"Screenshot saved to {file_name}\") # 출력\n",
    "    except:\n",
    "        print(\"에러 발생\")\n",
    "\n",
    "# 메인\n",
    "with mp_face_detection.FaceDetection(\n",
    "        model_selection=0, min_detection_confidence=0.5) as face_detection:\n",
    "    while cap.isOpened():\n",
    "        success, image = cap.read()\n",
    "        if not success:\n",
    "            break\n",
    "                \n",
    "        # To improve performance, optionally mark the image as not writeable to\n",
    "        # pass by reference.\n",
    "        image.flags.writeable = False\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        results = face_detection.process(image)\n",
    "\n",
    "        # Draw the face detection annotations on the image. 점을 그리는 함수\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        if results.detections:\n",
    "            for detection in results.detections:\n",
    "                pass\n",
    "                # mp_drawing.draw_detection(image, detection)\n",
    "                \n",
    "        #특정 위치 가져오기\n",
    "        keypoints = detection.location_data.relative_keypoints\n",
    "        right_eye = keypoints[0] # 오른쪽 눈\n",
    "        left_eye = keypoints[1] # 왼쪽 눈\n",
    "        nose_tip = keypoints[2] # 코 끝 부분\n",
    "        \n",
    "        # 이미지 위치 지정\n",
    "        right_eye = (int(right_eye.x * w)-20, int(right_eye.y * h)-100)\n",
    "        left_eye = (int(left_eye.x * w)+20, int(left_eye.y * h)-100) \n",
    "        nose_tip = (int(nose_tip.x * w), int(nose_tip.y * h)+30)\n",
    "        \n",
    "        # 이미지 대입\n",
    "        overlay(image, *right_eye, 25, 25, image_right_eye)\n",
    "        overlay(image, *left_eye, 25, 25, image_left_eye)\n",
    "        overlay(image, *nose_tip, 50, 50, image_nose_tip)\n",
    "        \n",
    "        # 영상 출력\n",
    "        # Flip the image horizontally for a selfie-view display.\n",
    "#         cv2.imshow('MediaPipe Face Detection', cv2.flip(image, 1)) # 좌우 반전되어 출력\n",
    "        cv2.imshow('MediaPipe Face Detection', cv2.resize(cv2.flip(image, 1), None, fx=1.5, fy=1.5))\n",
    "    \n",
    "        # 키보드 입력\n",
    "        keycode = cv2.waitKey(1) # 입력 값을 이런식으로 변수에 저장해서 사용해야 딜레이가 생기지 않음\n",
    "        \n",
    "        #q를 누르면 종료\n",
    "        if keycode == ord('q'):\n",
    "            break\n",
    "        \n",
    "        # 이미지 변환\n",
    "        if keycode == ord('a'):\n",
    "            image_right_eye = imageList[0][0]\n",
    "            image_left_eye = imageList[0][1]\n",
    "            image_nose_tip = imageList[0][2]\n",
    "            print('판다')\n",
    "\n",
    "        if keycode == ord('s'):\n",
    "            image_right_eye = imageList[1][0]\n",
    "            image_left_eye = imageList[1][1]\n",
    "            image_nose_tip = imageList[1][2]\n",
    "            print('고양이')\n",
    "\n",
    "        if keycode == ord('d'):\n",
    "            image_right_eye = imageList[2][0]\n",
    "            image_left_eye = imageList[2][1]\n",
    "            image_nose_tip = imageList[2][2]\n",
    "            print('개')\n",
    "        \n",
    "        # 화면 캡처\n",
    "        if keycode == ord('p'):\n",
    "            displayCapture(image)\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557b270c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 # opencv, 영상처리\n",
    "import mediapipe as mp # mediopipe, 얼굴 인식 라이브러리 사용\n",
    "import os # 파일 생성을 위해 사용\n",
    "import datetime # 파일 이름을 지정하기 위해 그리고 최대 녹화시간을 카운트 하기 위해 사용\n",
    "import pyaudio # 오디오 녹화\n",
    "import wave # 오디오 녹화\n",
    "import numpy as np # 오디오 연산, 표정 인식 얼굴 크기 계산\n",
    "from moviepy.editor import * # 비디오, 오디오 합성\n",
    "import time # 음성 파일이 생성 될 때 까지 delay 발생시키기 위해 사용\n",
    "import threading # 쓰레드, 두 가지 이상의 일을 하기 위해 사용 (opencv를 이용한 영상처리, moviepy를 이용한 영상, 음성 합성 동시 처리)\n",
    "import dlib\n",
    "from keras.models import load_model\n",
    "\n",
    "# mediapipe library 중에서 얼굴 인식 설정\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# 동물 이미지 기본 주소\n",
    "animal_path = 'C:/SEnowImage/'\n",
    "\n",
    "# 이미지, 영상 저장 기본 주소\n",
    "save_path = 'C:/senow/'\n",
    "\n",
    "# 웹캠 객체\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# 동물 필터 1, 표정 인식 필터 0\n",
    "af = 1\n",
    "\n",
    "# 얼굴 인식\n",
    "face_cascade = cv2.CascadeClassifier('c:/j/haarcascade_frontalface_default.xml')\n",
    "\n",
    "# 표정 인식을 위한 눈, 코, 입등의 위치 반환\n",
    "predictor = dlib.shape_predictor('c:/j/shape_predictor_68_face_landmarks.dat')\n",
    "\n",
    "# 표정 라벨링\n",
    "expression_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "\n",
    "# 표정 가중치 모델\n",
    "model = load_model('c:/j/emotion_model.hdf5')\n",
    "\n",
    "expression_label = None\n",
    "\n",
    "lowerb1 = (0, 40, 0)\n",
    "upperb1 = (20, 180, 255)\n",
    "\n",
    "# 렌즈 변수\n",
    "exp = 2       # 볼록, 오목 지수 (오목 : 0.1 ~ 1, 볼록 : 1.1~)\n",
    "scale = 1           # 변환 영역 크기 (0 ~ 1)\n",
    "\n",
    "# 창 크기 출력\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "print(width, height) # 640, 480\n",
    "SCREEN_REGION = (0, 0, width, height)\n",
    "\n",
    "# 텍스트\n",
    "org = (width-100, 30) # 위치\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX # 폰트\n",
    "scale = 1 # 크기\n",
    "color = (255, 0, 0) # 색상\n",
    "thickness = 2 # 굵기\n",
    "\n",
    "# 영상\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID') # 영상 코덱 설정\n",
    "out = None # 영상 저장 객체 초기화\n",
    "fps = 22.0 # 영상 프레임\n",
    "\n",
    "# 음성\n",
    "chunk = 1024 # 청크 크기\n",
    "format = pyaudio.paInt16 # 오디오 포맷 설정\n",
    "channels = 2 # 스테레오 (2), 모노(1)\n",
    "rate = 22050 # 샘플 레이트 설정, 44100으로하면 2배속으로 되서 22050 으로 했더니 정배속으로 됨\n",
    "duration = 10 # 녹음 최대 시간\n",
    "audio_frames = []\n",
    "audio = pyaudio.PyAudio() # 음성 객체 초기화\n",
    "stream = None \n",
    "\n",
    "# 녹화중인지 아닌지 여부를 저장할 변수\n",
    "recording = False\n",
    "\n",
    "# 파일 이름 저장 변수 (현재 시간을 이름으로 지정)\n",
    "fileName = \"\"\n",
    "\n",
    "\n",
    "#region 녹화 종료 클래스\n",
    "class record():\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def exit(out, stream, audio, audio_frames):\n",
    "        global recording\n",
    "        out.release()\n",
    "        stream.stop_stream()\n",
    "        stream.close()\n",
    "        audio.terminate()\n",
    "        \n",
    "        waveFile = wave.open(save_path + f\"{fileName}.wav\", 'wb')\n",
    "        waveFile.setnchannels(channels)\n",
    "        waveFile.setsampwidth(audio.get_sample_size(format))\n",
    "        waveFile.setframerate(rate)\n",
    "        waveFile.writeframes(b''.join(audio_frames))\n",
    "        waveFile.close()\n",
    "        audio_frames = []\n",
    "        recording = False\n",
    "        print('녹화종료')\n",
    "#endregion\n",
    "\n",
    "#region 영상, 소리 합성, 저장 함수\n",
    "def videoCapture():\n",
    "    print('영상, 소리 합성중')\n",
    "    # 음성 파일은 녹화가 끝나고 만들어 지므로 살짝 지연\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # 합성할 비디오, 오디오 객체 생성\n",
    "    videoclip = VideoFileClip(save_path + f\"{fileName}.avi\")\n",
    "    audioclip = AudioFileClip(save_path + f\"{fileName}.wav\")\n",
    "\n",
    "    # 비디오에 오디오 삽입후 파일 생성\n",
    "    videoclip.audio = audioclip\n",
    "    video = f\"{fileName}.mp4\"\n",
    "    videoclip.write_videofile(save_path + video)\n",
    "    print(f'영상, 소리 합성 완료, {video} 생성 완료')\n",
    "#endregion\n",
    "\n",
    "#region 이미지 저장 함수\n",
    "def displayCapture(image):\n",
    "    \n",
    "    # 저장 폴더, 없는 경우 생성\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "\n",
    "    try:\n",
    "        # 현재 시간을 파일 이름으로 지정하여 png 파일로 저장\n",
    "        current_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H-%M-%S\")\n",
    "        file_name = f\"{save_path}/{current_time}.png\"\n",
    "        cv2.imwrite(file_name, image) # 이미지 저장\n",
    "        print(f\"{file_name} 저장 완료\") # 출력\n",
    "    except:\n",
    "        print(\"에러 발생\")\n",
    "#endregion \n",
    "\n",
    "#region 필터 이미지 적용 함수\n",
    "def filter_images(name):\n",
    "    global image_right_eye, image_left_eye, image_nose_tip, imageList\n",
    "    image_right_eye = imageList[name][0]\n",
    "    image_left_eye = imageList[name][1]\n",
    "    image_nose_tip = imageList[name][2]\n",
    "#endregion\n",
    "\n",
    "#region 볼록 렌즈 필터 함수\n",
    "def lens(exp, scale, frame):\n",
    "    rows, cols = frame.shape[:2]\n",
    "    # 매핑 배열 생성 ---②\n",
    "    mapy, mapx = np.indices((rows, cols),dtype=np.float32)\n",
    "    # 좌상단 기준좌표에서 -1~1로 정규화된 중심점 기준 좌표로 변경 ---③\n",
    "    mapx = 2*mapx/(cols-1)-1\n",
    "    mapy = 2*mapy/(rows-1)-1\n",
    "    # 직교좌표를 극 좌표로 변환 ---④\n",
    "    r, theta = cv2.cartToPolar(mapx, mapy)\n",
    "    # 왜곡 영역만 중심확대/축소 지수 적용 ---⑤\n",
    "    r[r< scale] = r[r<scale] **exp  \n",
    "    # 극 좌표를 직교좌표로 변환 ---⑥\n",
    "    mapx, mapy = cv2.polarToCart(r, theta)\n",
    "    # 중심점 기준에서 좌상단 기준으로 변경 ---⑦\n",
    "    mapx = ((mapx + 1)*cols-1)/2\n",
    "    mapy = ((mapy + 1)*rows-1)/2\n",
    "    return mapx, mapy\n",
    "#endregion\n",
    "\n",
    "# 비디오와 오디오를 합칠때 thread를 쓰지 않으면 opencv가 끊기므로 thread 적용\n",
    "video_save = threading.Thread(target=videoCapture)\n",
    "\n",
    "#region 동물 이미지 불러오기\n",
    "# cv2.IMREAD_UNCHANGED, 이미지파일을 alpha channel(누끼)까지 포함하여 읽는다.\n",
    "# 이미지를 쉽게 보기 위해 dictionary 를 사용\n",
    "imageList = {\n",
    "    'panda' : [cv2.imread(animal_path+'panda/right_eye_cutout.png', cv2.IMREAD_UNCHANGED), cv2.imread(animal_path+'panda/left_eye_cutout.png', cv2.IMREAD_UNCHANGED),\n",
    "            cv2.imread(animal_path+'panda/nose_tip_cutout.png', cv2.IMREAD_UNCHANGED)],\n",
    "    'cat' : [cv2.imread(animal_path+'cat/right_eye2.png', cv2.IMREAD_UNCHANGED), cv2.imread(animal_path+'cat/left_eye2.png', cv2.IMREAD_UNCHANGED),\n",
    "            cv2.imread(animal_path+'cat/nose_tip2.png', cv2.IMREAD_UNCHANGED)],\n",
    "    'dog' : [cv2.imread(animal_path+'dog/right_eye3.png', cv2.IMREAD_UNCHANGED), cv2.imread(animal_path+'dog/left_eye3.png', cv2.IMREAD_UNCHANGED),\n",
    "            cv2.imread(animal_path+'dog/nose_tip3.png', cv2.IMREAD_UNCHANGED)]\n",
    "}\n",
    "#endregion\n",
    "\n",
    "# 처음 이미지 기본값, 판다 적용\n",
    "image_right_eye = imageList['panda'][0]\n",
    "image_left_eye = imageList['panda'][1]\n",
    "image_nose_tip = imageList['panda'][2]\n",
    "\n",
    "#region mideapipe 얼굴인식 기본 코드\n",
    "IMAGE_FILES = []\n",
    "with mp_face_detection.FaceDetection(\n",
    "        model_selection=1, min_detection_confidence=0.5) as face_detection:\n",
    "    for idx, file in enumerate(IMAGE_FILES):\n",
    "        image = cv2.imread(file)\n",
    "        # Convert the BGR image to RGB and process it with MediaPipe Face Detection.\n",
    "        results = face_detection.process(\n",
    "            cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "        # Draw face detections of each face.\n",
    "        if not results.detections:\n",
    "            continue\n",
    "        annotated_image = image.copy()\n",
    "        for detection in results.detections:\n",
    "            print('Nose tip:')\n",
    "            print(mp_face_detection.get_key_point(\n",
    "                detection, mp_face_detection.FaceKeyPoint.NOSE_TIP))\n",
    "            mp_drawing.draw_detection(annotated_image, detection)\n",
    "        cv2.imwrite('/tmp/annotated_image' +\n",
    "                    str(idx) + '.png', annotated_image)\n",
    "#endregion\n",
    "\n",
    "#region 캠에 이미지 덮어 씌우는 함수\n",
    "def overlay(image, x, y, w, h, overlay_image): # 대상 이미지, x, y 좌표, width, height, 덮어씌울 이미지\n",
    "    alpha = overlay_image[:, :, 3] #BGRA, A값을 가져옴\n",
    "    mask_image = alpha / 255 # 0~255 ->255로 나누면 0~1의 값을 가짐, 1: 불투명, 0: 투명\n",
    "    # print(x, y, w, h)\n",
    "    \n",
    "    # 얼굴이 창 크기를 벗어나면 오류가 생기므로 예외처리\n",
    "    try:\n",
    "        for c in range(0, 3): #BGR 처리\n",
    "             image[y-h: y+h, x-w: x+w, c] = (overlay_image[:, :, c] * mask_image) + (image[y-h: y+h, x-w: x+w, c] * (1-mask_image))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        pass\n",
    "#endregion\n",
    "\n",
    "#region 메인 실행 코드\n",
    "with mp_face_detection.FaceDetection(\n",
    "        model_selection=0, min_detection_confidence=0.5) as face_detection:\n",
    "    \n",
    "    animal = 'panda'\n",
    "    while cap.isOpened():\n",
    "        ret, image = cap.read()\n",
    "        \n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # 영상 좌우반전\n",
    "        # 스마트폰의 전면 카메라 처럼 카메라 적용\n",
    "        image = cv2.flip(image, 1)\n",
    "        \n",
    "        # 현재시간 표시\n",
    "        now = datetime.datetime.now().strftime(\"%H-%M-%S\")\n",
    "        time_org = (10, 30)\n",
    "        # cv2.putText(image, str(now), time_org, font, scale, color, thickness)\n",
    "        \n",
    "        # 현재 적용된 동물 필터 텍스트 출력\n",
    "        # cv2.putText(image, animal, org, font, scale, color, thickness)\n",
    "        \n",
    "        # To improve performance, optionally mark the image as not writeable to\n",
    "        # pass by reference.\n",
    "        image.flags.writeable = False\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        results = face_detection.process(image)\n",
    "\n",
    "        if af:\n",
    "            # Draw the face detection annotations on the image. \n",
    "            # mediapipe의 얼굴을 rectagle로 나타내고 세부 위치에 dot로 그리는 함수 부분\n",
    "            # dot와 rectagle을 그리는 대신 점의 좌표를 이용해 동물 이미지를 추가하도록 수정\n",
    "            image.flags.writeable = True\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "            if results.detections:\n",
    "                for detection in results.detections:\n",
    "                    # mp_drawing.draw_detection(image, detection) # 얼굴 크기에 맞게 박스를 생성\n",
    "                    # print(detection) # detection의 값을 보기위해 사용\n",
    "                    \n",
    "                    # dot의 특정 위치 가져오기\n",
    "                    keypoints = detection.location_data.relative_keypoints\n",
    "                    right_eye = keypoints[0] # 오른쪽 눈\n",
    "                    left_eye = keypoints[1] # 왼쪽 눈\n",
    "                    nose_tip = keypoints[2] # 코 끝 부분\n",
    "                    \n",
    "                    # 이미지 적용\n",
    "                    # 이미지의 크기를 동적으로 하기위해 얼굴의 크기 값을 연산하는 부분을 이용\n",
    "                    box_wh = detection.location_data.relative_bounding_box\n",
    "                    box_w = int(round(box_wh.width, 2) * 100)\n",
    "                    box_h = int(round(box_wh.height, 2) * 100)\n",
    "\n",
    "                    # 이미지 위치 지정, 얼굴크기에 맞게 위치를 동적으로 지정 함\n",
    "                    w, h = width, height\n",
    "                    right_eye = (int(right_eye.x * w)-box_w, int(right_eye.y * h)-(box_h*3))\n",
    "                    left_eye = (int(left_eye.x * w)+box_w, int(left_eye.y * h)-(box_h*3)) \n",
    "                    nose_tip = (int(nose_tip.x * w), int(nose_tip.y * h)+box_h)\n",
    "                    \n",
    "                    #region 이미지 대입\n",
    "                    # operands could not be broadcast together with shapes을 방지하기 위해 기존 이미지를 변형 한 후 사용\n",
    "                    # 해당 에러는 현재 입히는 이미지의 크기와 opencv 상에서 적용되는 이미지의 해상도가 달라 생기는 것으로\n",
    "                    # 둘의 이미지를 같아지게 하도록 cv2.resize()를 적용\n",
    "                    \n",
    "                    # 오른쪽 귀\n",
    "                    overlay_right_eye = cv2.resize(image_right_eye, (box_w*2, box_h*2))\n",
    "                    overlay(image, *right_eye, box_w, box_h, overlay_right_eye)\n",
    "                    \n",
    "                    # 왼쪽 귀\n",
    "                    overlay_left_eye = cv2.resize(image_left_eye, (box_w*2, box_h*2))\n",
    "                    overlay(image, *left_eye, box_w, box_h, overlay_left_eye)\n",
    "                    \n",
    "                    # 코, 입\n",
    "                    overlay_nose_tip = cv2.resize(image_nose_tip, (box_w*4, box_h*4))\n",
    "                    overlay(image, *nose_tip, box_w*2, box_h*2, overlay_nose_tip)\n",
    "                    #endregion\n",
    "        else:\n",
    "            # 얼굴인식을 위해 gray 변환\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "            \n",
    "            # 얼굴 인식\n",
    "            # scaleFactor이 1에 가까울수록 표정 인식이 잘 되고 멀 수록 잘 안됨\n",
    "            faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "            \n",
    "            #region 얼굴이 인식되면 표정을 인식\n",
    "            for (x, y, w, h) in faces:\n",
    "                # 얼굴 크기에 알맞도록 사각형 그리기\n",
    "                # cv2.rectangle(image, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "                \n",
    "                # 얼굴 크기 반환\n",
    "                face_roi = gray[y:y+h, x:x+w]\n",
    "\n",
    "                # 표정을 인식하기 위해 표정 dataset과 똑같은 사이즈 변환\n",
    "                # dataset 이미지와 입력된 얼굴의 크기가 다르면 error 발생\n",
    "                face_roi = cv2.resize(face_roi, (64, 64))\n",
    "                face_roi = np.expand_dims(face_roi, axis=-1)\n",
    "                face_roi = np.expand_dims(face_roi, axis=0)\n",
    "                face_roi = face_roi / 255.0\n",
    "\n",
    "                # 모델을 통해 표정 분석\n",
    "                output = model.predict(face_roi)[0]\n",
    "\n",
    "                # 해당 표정의 값 반환\n",
    "                expression_index = np.argmax(output)\n",
    "\n",
    "                # 표정에 따른 label 값 저장\n",
    "                expression_label = expression_labels[expression_index]\n",
    "\n",
    "                # 표정 값 출력\n",
    "                print(expression_label, end=' ')\n",
    "                # cv2.putText(frame, expression_label, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "            \n",
    "            #endregion\n",
    "            \n",
    "            # region 표정에 따른 필터\n",
    "            hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "            if expression_label == 'Surprise' or expression_label == 'Fear':\n",
    "                # mask = cv2.inRange(hsv, lowerb1, upperb1)\n",
    "                # frame = mask # 2차원 형태로 얼굴의 형태만 추출\n",
    "                # frame = cv2.bitwise_and(frame, frame, mask=mask) # 검출된 얼굴의 영역을 원본 이미지에 합성\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "                \n",
    "            mapx, mapy = lens(exp, scale, image)\n",
    "            if expression_label == 'Happy':\n",
    "                image = cv2.remap(image, mapx, mapy, cv2.INTER_LINEAR)\n",
    "            #endregion\n",
    "\n",
    "        # 영상 출력\n",
    "        cv2.imshow('SEnow Camera', cv2.resize(image, None, fx=1.5, fy=1.5))\n",
    "\n",
    "        # 키보드 입력\n",
    "        keycode = cv2.waitKey(25)\n",
    "        \n",
    "        #esc 를 누르면 종료\n",
    "        if keycode == 27:\n",
    "            break\n",
    "        \n",
    "        #region 필터 변환\n",
    "        if keycode == ord('j') and af == 0:\n",
    "            af = 1\n",
    "        \n",
    "        if keycode == ord('k') and af:\n",
    "            af = 0\n",
    "        #endregion\n",
    "        \n",
    "        #region 이미지 변환\n",
    "        if keycode == ord('a'):\n",
    "            animal = 'panda'\n",
    "            filter_images(animal)\n",
    "            print('판다')\n",
    "\n",
    "        if keycode == ord('s'):\n",
    "            animal = 'cat'\n",
    "            filter_images(animal)\n",
    "            print('고양이')\n",
    "\n",
    "        if keycode == ord('d'):\n",
    "            animal = 'dog'\n",
    "            filter_images(animal)\n",
    "            print('개')\n",
    "        #endregion\n",
    "        \n",
    "        # opencv 사진 저장\n",
    "        if keycode == ord('p'):\n",
    "            # 이미지라서 args를 튜플 형식이 아닌 리스트 형식으로 값을 넣어줘야 된다.\n",
    "            img_save = threading.Thread(target=displayCapture, args=[image])\n",
    "            img_save.start()\n",
    "        \n",
    "        #region 녹화 시작\n",
    "        if keycode == ord('v') and recording == False:\n",
    "            print('녹화 시작')\n",
    "            # 오디오 생성\n",
    "            stream = audio.open(format=format, channels=channels, rate=rate, input=True, frames_per_buffer=chunk)\n",
    "            \n",
    "            # 비디오 저장 객체 생성\n",
    "            fileName = datetime.datetime.now().strftime(\"%Y-%m-%d %H-%M-%S\")\n",
    "            out = cv2.VideoWriter(save_path + f'{fileName}.avi',fourcc, fps, (width, height))\n",
    "\n",
    "            # 녹화 시작\n",
    "            recording = True\n",
    "            start_time = datetime.datetime.now()\n",
    "        #endregion\n",
    "        \n",
    "        # 녹음 시간이 duration 을 넘으면 녹화 종료\n",
    "        if keycode == ord('b') and recording:\n",
    "            # 녹화 종료\n",
    "            record.exit(out, stream, audio, audio_frames)\n",
    "            video_save.start()\n",
    "        \n",
    "        #region 녹화 중이면\n",
    "        if recording:\n",
    "            \n",
    "            # 프레임 녹화\n",
    "            out.write(image)\n",
    "            \n",
    "            # 음원 녹화\n",
    "            audio_frames.append(np.frombuffer(stream.read(chunk), dtype=np.int16))\n",
    "            \n",
    "            # 녹음 시간이 duration 을 넘으면 녹화 종료\n",
    "            if (datetime.datetime.now() - start_time).seconds >= duration:\n",
    "                # 녹화 종료\n",
    "                record.exit(out, stream, audio, audio_frames)\n",
    "                video_save.start()\n",
    "        #endregion\n",
    "#endregion\n",
    "\n",
    "if cap.isOpened():\n",
    "    cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "9650cb4e16cdd4a8e8e2d128bf38d875813998db22a3c986335f89e0cb4d7bb2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
